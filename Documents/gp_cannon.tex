\documentclass[12pt, preprint]{aastex}
\usepackage{bm, graphicx, subfigure, amsmath, morefloats}
\bibliographystyle{apj}

% naming macros
\newcommand{\tc}{\textsl{The~Cannon}}
\newcommand{\apogee}{\textsl{APOGEE}}

% math and symbol macros
\newcommand{\set}[1]{\bm{#1}}
\newcommand{\starlabel}{\ell}
\newcommand{\starlabelvec}{\set{\starlabel}}
\newcommand{\mean}[1]{\overline{#1}}
\newcommand{\given}{\,|\,}
\newcommand{\teff}{\mbox{$\rm T_{eff}$}}
\newcommand{\kms}{\mbox{$\rm kms^{-1}$}}
\newcommand{\feh}{\mbox{$\rm [Fe/H]$}}
\newcommand{\xfe}{\mbox{$\rm [X/Fe]$}}
\newcommand{\alphafe}{\mbox{$\rm [\alpha/Fe]$}}
\newcommand{\mh}{\mbox{$\rm [M/H]$}}
\newcommand{\logg}{\mbox{$\rm \log g$}}
\newcommand{\noise}{\sigma_{n\lambda}}
\newcommand{\scatter}{s_{\lambda}}
\newcommand{\pix}{\mathrm{pix}}
\newcommand{\rfn}{\mathrm{ref}}

\begin{document}

\title{\tc\ in the Gaussian Process Framework: \\ Data-driven spectral
model determination}
\author{A.Y.Q.~Ho\altaffilmark{1},
D.~Foreman-Mackey\altaffilmark{2},
M.~Ness\altaffilmark{1},
David~W.~Hogg\altaffilmark{1,2,3}, 
H.-W.~Rix\altaffilmark{1}
}
\altaffiltext{1}{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{2}{Center for Cosmology and Particle Physics, Department of Phyics,
New York University, 4 Washington Pl., room 424, New York, NY, 10003, USA}
\altaffiltext{3}{Center for Data Science, New York University, 726 Broadway, 7th Floor, New York, NY 10003, USA}

\email{annaho@mpia.de}

\begin{abstract}

\tc\ is a data-driven method for determining stellar labels (parameters and 
abundances) from stellar spectra in the context of vast spectroscopic surveys. 
For a ``training set'' of stars with known labels, \tc\ fits for a spectral 
model that describes how the flux in each pixel of the spectrum depends on the 
labels of the star. The model is then applied to the rest of the stars in the 
survey to solve for their labels. 
In the first iteration of \tc\ \ref{ness2015} the functional form of the 
spectral model was determined through empirical experimentation and held fixed
across the spectrum: only the coefficients varied between pixels. 
This constituted a severe limitation: the functional form of the model was too 
flexible for some pixels 
not flexible enough for others. In this paper, we cast \tc\ into the framework
of a Gaussian Process. In this framework, instead of optimizing for 
coefficients of a fixed spectral model at each pixel, we optimize for the
functional form of the spectral model at each pixel. 

\end{abstract}

\keywords{
methods: data analysis
---
methods: statistical
---
stars:abundances
---
stars: fundamental parameters
---
surveys
---
techniques: spectroscopic}

\section{Introduction}

\tc\ is a data-driven method for determining stellar labels (parameters and 
abundances) from stellar spectra in the context of vast spectroscopic surveys. 
For a ``training set'' of stars with known labels, \tc\ fits for a spectral 
model that describes how the flux in each pixel of the spectrum depends on the 
labels of the star. The model is then applied to the rest of the stars in the 
survey to solve for their labels. 
In the first iteration of \tc\ \ref{ness2015} the functional form of the 
spectral model was determined through empirical experimentation and held fixed
across the spectrum: only the coefficients varied between pixels. 

More specifically, we took the 
spectral model to be characterized by a coefficient vector $\theta_\lambda$ 
that allowed us to predict the flux at every pixel $f_{n \lambda}$ for a 
given label vector $\textbf{l}_n$:

\begin{equation}
  f_{n\lambda} = g(\starlabelvec_n | \set{\theta}_\lambda) + \mbox{noise} 
\end{equation}

\begin{equation}
  f_{n\lambda} = \set{\theta}_\lambda^T \cdot \starlabelvec_n + \mbox{noise}  
\end{equation}

In this case, we optimized for coefficients of a model that was quadratic in the
labels, such that the label vector was,

\begin{equation}
  \starlabelvec_n \equiv 
  [1, \teff, \logg, \feh, \teff^2, \teff\cdot\logg, \teff\cdot\feh, \logg^2, \logg\cdot\feh, \feh^2]
\end{equation}

A major limitation of this existing framework is that the model is inflexible:
although a set of coefficients is uniquely determined for each pixel, 
the functional form of the model itself is held fixed across pixels. 
This, however, is not physically motivated. Indeed, the
polynomial family is probably not the best family of functions to be 
exploring, since they extrapolate badly (edge effects) and require explicit, 
qualitative choices about order and cross-terms. In that paper, we mentioned 
the hope to eventually move to a non-parametric form for the functions, 
such that model complexity would be controlled by continuous parameters
and at each pixel the functional form would only be complex as warranted
by the training data.

To give a more specific example, different pixels should have 
different functional dependencies on labels: for example, one might expect a 
continuum pixel to be linear, while a pixel in a magnesium line should show 
polynomial (?) dependence on temperature. In particular, absorption features,
particularly strong lines, are known to vary non-linearly as a function of 
stellar labels. And, indeed, in the paper, systematic discrepencies were found. 

Extending \emph{The Cannon} to a Gaussian process framework is one way of 
allowing the training data to determine the functional form and complexity
of the model at each pixel. We now provide a brief introduction to Gaussian
Processes: an excellent, more detailed review can be found here and here. 

A Gaussian Process is a probability distribution over functions. More
precisely, it is a collection of random variables, any finite number of which
have a joint Gaussian distribution. 
It is an example of a stochastic process: the generalization
of a probability distribution (which describes a finite-dimensional random
variable) to function space. Using some set of finite training data, our goal
is to solve for a function that can make a predictions for all possible
input values: this is regression, input-output mapping (learning) for 
continuous inputs. This is a problem of induction: moving from points you
know to points you don't know. It basically entails doing inference in 
function space. 

In order to fit for the function, you need to begin by making some assumptions 
about the characteristics of the 
underlying function. This entails assigning a prior probability to every 
possible function, essentially specifying a probability distribution over
the functions themselves, by specifying a mean and covariance function. 
This is a prior distribution over the functions.
It turns out that you only need to deal with
some of the points in a function governed by a Gaussian distribution in order
to get the answer that you would if you had access to all of the points. 
This computational tractability is a big bonus of Gaussian processes. 
And at the end you get 
a mean prediction and a posterior distribution.

The basic structure of \tc\ remains unchanged: we still have a training step
in which we use a set of training objects to fit for a model, and then a test
step in which we apply that model to solve for labels of the test objects. 
Below we walk through what it 
means to cast each step into the Gaussian Process framework.

\section{\emph{The Cannon} Training Step in the Gaussian Process Framework}

For each pixel, we optimize for a set of five hyperparameters that fully 
describe the covariance matrix that characterizes the Gaussian process model.

We begin by selecting the functional form of the covariance between any 
two outputs (in this case, between the flux value at a given pixel for any
two objects, across
label space). We choose a squared exponential covariance function. 

For a set of $N_{ref}$ reference objects $n_1, n_2, \dots, n_{N_{ref}}$, 
each has a continuum-normalized
flux measurement $f_{n \lambda}$ at wavelength $\lambda$ and a set of  
$K$ training labels $l_{nk}$ 
Say we have a set of $N_{ref}$ training objects, each with $k$ training labels 
$l = l_{nk_1}, l_{nk_2}, \cdots, l_{nk_K}$.

At any given pixel, the covariance between any pair of flux values for two
stars $n_i$ and $n_j$ with label sets $l_{n_i k}$ and $l_{n_j k}$, 
$ f_{n_i \lambda}$ and $f_{n_j \lambda}$ across label space is: 

\begin{equation}
  Cov(f_{n_i \lambda}, f_{n_j \lambda}) = 
  a_\lambda^2 \exp(-\frac{1}{2} 
  \sum_{k=1}^K \frac{(l_{n_i k}-l_{n_j k})^2}{\tau_{\lambda k}^2}) +
  \delta_{ij} s_\lambda^2
\end{equation}

More compactly, the covariance matrix $\Sigma$ can be written as follows:

\begin{equation}
  \Sigma_{ij} = 
  a_\lambda^2 \exp(-\frac{1}{2} 
  \sum_{k=1}^K \frac{(l_{i k}-l_{j k})^2}{\tau_{\lambda k}^2}) +
  \delta_{ij} s_\lambda^2
\end{equation}

Note a few things. First, the covariance between outputs is a function of the 
input labels. Second, this covariance approaches unity as the points become 
closer together in label space and becomes smaller as the distance between 
the points increases. Thus the variable $\tau$ can informally be thought of 
as a length scale over which flux values become more ``different`` or contain 
less information about each other.

In this equation, the pixel-dependent values $a, \tau_k, $ and $s$ constitute
a set of hyperparameters $\theta$ that determine a Gaussian process model. Thus, 
each pixel has its own Gaussian process model characterized by hyperparameters,
the number of which depends on the number of labels. For three labels, then,
the set of hyperparameters at a given pixel would look like this:

\begin{equation}
  \theta_\lambda = a_\lambda, 
  \tau_{\lambda 0}, \tau_{\lambda 1}, \tau_{\lambda 2}, s_\lambda
\end{equation}

In a nutshell, we optimize by maximizing the log marginal likelihood for a 
single pixel across all objects. The mathematics of how to do so is laid out 
below\ldots

The marginal likelihood $p(g|X)$ is the integral of the likelihood times the
prior. It refers to the marginalization over the function values g.  

\begin{equation}
  p(f|X) = \int p(f|g,X) p(g|X)dg
  \label{}
\end{equation}

For the special case of a Gaussian process model, we have that 

\begin{equation}
  \ln{p}(\textbf{f} | \theta, X) = 
  -\frac{1}{2} \textbf{f}^{T}\,\Sigma^{-1}\,\textbf{f}-\frac{1}{2}\ln{|\Sigma|}
\end{equation}

In practice, inverting the covariance matrix is computationally challenging.
It is faster and numerically more stable to avoid directly inverting the 
matrix: here, we use Cholesky decomposition and the associated routines
in the scipy linear algebra package. 

In Cholesky decomposition, we decompose the covariance matrix as follows:

\begin{equation}
  \Sigma = L^T L  
  \label{}
\end{equation}

where L is called the Cholesky factor. We also introduce $\alpha$, 

\begin{equation}
  \Sigma \, \alpha = \textbf{f}
  \label{}
\end{equation}

such that the expression for the log likelihood can be written as follows:

\begin{equation}
  \ln{p}(\textbf{f} | \theta, X) = 
  -\frac{1}{2} \textbf{f}^T \alpha - \sum_i \ln{L_{ii}}
\end{equation}

We use the scipy optimize routine to find, for each pixel, the set of
hyperparameters (in this case, five) that maximize the log likelihood. 
In other words, we're looking for the hyperparameters that make it most likely
that this set of flux value and training label values would be measured.

Once we have those hyperparameters, the training step is over: we now have
a Gaussian Process model at each pixel of the spectrum. 


\section{\emph{The Cannon} Test Step in the Gaussian Process Framework}

While the training step took place across pixel space, optimizing for 
hyperparameters across objects at each pixel independently, the test step 
will take place across object space, optimizing for labels across pixels at 
each object independently.

Now, we have a set of $(N_{ref}, K)$ hyperparameters where $N_{ref}$ is 
the number of training objects and $K$ is the number of training labels 
describing each object.


In the end, we get a mean function and a variance function. 

We have a vector of covariances between the test point and the training 
points. Denote this $\Sigma(X,X_*)$. Then the mean function is

\begin{equation}
  \bar{g}_* = \Sigma(X_*, X)\,\Sigma(X, X)^{-1}\,f
  \label{}
\end{equation}

which, in the $\alpha$ and $L$ framework we adopt for computational
tractability, is

\begin{equation}
  \bar{g}_* = \Sigma(X, X_*)\,\alpha
  \label{}
\end{equation}

and the covariance array corresponding to the Gaussian process model is

\begin{equation}
  \sigma_* = \Sigma(X_*, X_*)-\Sigma(X_*,X)\,\Sigma(X,X)^{-1}\,K(X,X_*)
  \label{}
\end{equation}

For some reason, that first term is actually just 
$\Sigma(X_*, X_*) = a^2 + \sigma$ where $\sigma$ here is the uncertainties
in the fluxes in the data. 

\begin{equation}
  \sigma_* = \sigma_f + a^2 - \Sigma(X_*,X) (L^T)^{-1}L^{-1} K_*[0]
  \label{}
\end{equation}

Now that we have both the mean and variance function, 
we can write a test log likelihood function that optimizes over the full pixel
and 3D label space in order to determine, for a particular object, which 
combination of labels is most likely given the measured flux at that pixel.

\begin{equation}
  \ln{p(f|l)} = -\frac{1}{2} \frac{(f-\bar{g}_*)^2}{\sigma_*^2} - \ln{\sigma_*^2}
  \label{}
\end{equation}

We perform the optimization for each object individually, across all pixels,
using the scipy.optimize minimize routine. 

\section{Results: Comparison with Quadratic Model}

\section{Discussion}

One challenge is the computational time. Much slower than simply choosing 
a quadratic model. The training steps scales 
with the cube of the number of training objects. In this case, with 544 
training objects, it took 20 
hours on a 4-core processor, with the operations running in parallel (all 
pixels are independent.) 

Labels for the test objects can also be 
evaluated in parallel, and the test step took X hours on a 4-core processor. 
The matrix inversion is the time-consuming step, but as you can see from this
equation, this step only depends on the training set: thus, we can evalute
this for each pixel only once, before performing the optimization for each 
pixel. 

\section{Acknowledgements}

AYQH was partially supported by a Fulbright grant through the German-American
Fulbright Commission.

The research has received funding from the European Research Council under the
European Union's Seventh Framework Programme (FP 7) ERC Grant Agreement n.
[321035].

\begin{thebibliography}{24}

  \bibitem[{ {Ness}{ et~al.}(2015){Ness}, {Hogg}, {Rix}, {Ho}, \&
    {Zasowski}}]{ness2015}
    {Ness}, M., {Hogg}, D.W., {Rix}, H.-W., {Ho}, A.Y.Q., \& {Zasowski}, G. 2015

  \bibitem[{ {Rasmussen}{et~al.}(2006){Rasmussen}, \& {Williams}}]{rasmussen2006}
    {Rasmussen}, C.E., {Williams}, C.K.I. 2006

\end{thebibliography}

\end{document}


