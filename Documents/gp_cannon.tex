\documentclass[12pt, preprint]{aastex}
\usepackage{bm, graphicx, subfigure, amsmath, morefloats}
\bibliographystyle{apj}

% naming macros
\newcommand{\tc}{\textsl{The~Cannon}}
\newcommand{\apogee}{\textsl{APOGEE}}

% math and symbol macros
\newcommand{\set}[1]{\bm{#1}}
\newcommand{\starlabel}{\ell}
\newcommand{\starlabelvec}{\set{\starlabel}}
\newcommand{\mean}[1]{\overline{#1}}
\newcommand{\given}{\,|\,}
\newcommand{\teff}{\mbox{$\rm T_{eff}$}}
\newcommand{\kms}{\mbox{$\rm kms^{-1}$}}
\newcommand{\feh}{\mbox{$\rm [Fe/H]$}}
\newcommand{\xfe}{\mbox{$\rm [X/Fe]$}}
\newcommand{\alphafe}{\mbox{$\rm [\alpha/Fe]$}}
\newcommand{\mh}{\mbox{$\rm [M/H]$}}
\newcommand{\logg}{\mbox{$\rm \log g$}}
\newcommand{\noise}{\sigma_{n\lambda}}
\newcommand{\scatter}{s_{\lambda}}
\newcommand{\pix}{\mathrm{pix}}
\newcommand{\rfn}{\mathrm{ref}}

\begin{document}

\title{\tc\ in the Gaussian Process Framework: \\ Data-driven spectral
model determination}
\author{A.Y.Q.~Ho\altaffilmark{1},
D.~Foreman-Mackey\altaffilmark{2},
M.~Ness\altaffilmark{1},
David~W.~Hogg\altaffilmark{1,2,3}, 
H.-W.~Rix\altaffilmark{1}
}
\altaffiltext{1}{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{2}{Center for Cosmology and Particle Physics, Department of Phyics,
New York University, 4 Washington Pl., room 424, New York, NY, 10003, USA}
\altaffiltext{3}{Center for Data Science, New York University, 726 Broadway, 7th Floor, New York, NY 10003, USA}

\email{annaho@mpia.de}

\begin{abstract}

\tc\ is a data-driven method for determining stellar labels (parameters and 
abundances) from stellar spectra in the context of vast spectroscopic surveys. 
For a ``training set'' of stars with known labels, \tc\ fits for a spectral 
model that describes how the flux in each pixel of the spectrum depends on the 
labels of the star. The model is then applied to the rest of the stars in the 
survey to solve for their labels. 
In the first iteration of \tc\ (Ness et al. 2015) the functional form of the 
spectral model was determined through empirical experimentation and held fixed
across the spectrum: only the coefficients varied between pixels. 
This constitutes a severe limitation: the functional form of the model is too 
flexible for some pixels 
not flexible enough for others. In this paper, we cast \tc\ into the framework
of a Gaussian Process. In this framework, instead of optimizing for 
coefficients of a fixed spectral model at each pixel, we optimize for the
functional form of the spectral model at each pixel. 

\end{abstract}

\keywords{
methods: data analysis
---
methods: statistical
---
stars:abundances
---
stars: fundamental parameters
---
surveys
---
techniques: spectroscopic}

\section{Introduction}

\tc is a data-driven method for determining stellar labels (parameters and 
abundances) from stellar
spectra in the context of vast spectroscopic surveys. Arguably the most
exciting long-term prospect of \tc is its potential to bring qualitatively
different stellar surveys onto a consistent stellar parameter and chemical
abundance scale, given a set of reference objects observed in common between
the surveys. In the current framework, a model is fit for that describes
how the flux in each pixel of a continuum-normalized spectrum depends
on the labels of the star.

A major limitation of this existing framework is that the functional form of 
the model is restricted to being the same for each pixel. We took the 
spectral model to be characterized by a coefficient vector $\theta_\lambda$ 
that allowed us to predict the flux at every pixel $f_{n \lambda}$ for a 
given label vector $\textbf{l}_n$:

\begin{equation}
  f_{n\lambda} = g(\starlabelvec_n | \set{\theta}_\lambda) + \mbox{noise} 
\end{equation}

\begin{equation}
  f_{n\lambda} = \set{\theta}_\lambda^T \cdot \starlabelvec_n + \mbox{noise}  
\end{equation}

In this case, we optimize for coefficients of a model that is quadratic in the
labels, such that the label vector is,

\begin{equation}
  \starlabelvec_n \equiv 
  [1, \teff, \logg, \feh, \teff^2, \teff\cdot\logg, \teff\cdot\feh, \logg^2, \logg\cdot\feh, \feh^2]
\end{equation}

A major limitation of this existing framework is that the model is inflexible:
although a set of coefficients is uniquely determined for each pixel, 
the functional form of the model itself is held fixed across pixels. 
This is, however, not physically motivated.  Different pixels should have 
different functional dependencies on labels: for example, one might expect a 
continuum pixel to be linear, while a pixel in a magnesium line should show 
polynomial (?) dependence on temperature. In particular, absorption features,
particularly strong lines, are known to vary non-linearly as a function of 
stellar labels. Systematic discrepencies were found. Previously, we had to 
settle on a suitable functional form for the spectral model. Now, we don't have
to settle on a functional form: we allow the fitting process to optimize and
identify the functional form in a data-driven way.

We needed to pick a functional form for the spectral model, which we did
through empirical experimentation with a particular dataset. Indeed, the
polynomial family is probably not the best family of functions to be 
exploring, since they extrapolate badly (edge effects) and require explicit, 
qualitative choices about order and cross-terms. In that paper, we mentioned 
the hope to eventually move to a non-parametric form for the functions, such 
as Gaussian Processes. In this case, model complexity would be controlled by 
continuous parameters and the functional form would become very complex at 
the pixels where the data in the training step warrant it.

Extending \emph{The Cannon} to a Gaussian process framework entails fitting 
for a Gaussian process model at every pixel, which actually means optimizing 
for the \emph{functional form} of the model at every pixel instead of simply 
optimizing for parameters that specify one functional form.

Learning a function from examples. Stochastic processes: the generalization 
of a probability distribution (a finite-dimensional random variable) to 
functions. Regression: input-output mapping (learning) for continuous inputs. 
It's an inductive problem: you want to move from points you know, to points 
you don't know\ldotsfrom finite training data to a function that makes 
predictions for all possible input values.

Obviously you have to make some assumptions about the characteristics of the 
underlying function. What you do is: assign a prior probability to every 
possible function (essentially, you specify a probability distribution over 
functions themselves) The key is: if you consider such a stochastic process 
function (i.e. a function governed by a Gaussian distribution\ldots) then if 
you only work with its values at a finite number of points, you'll get the 
same answer as if you were dealing with all of the points.

So, one big bonus is computational tractability.

You begin with a prior distribution on the functions. And at the end you get 
a mean prediction and a posterior distribution.

Basically, you do inference directly in function space.

A Gaussian Process IS a distribution over functions. The distribution is 
specified by the covariance function. More formally, a Gaussian Process 
is a collection of random variables, any finite number of which have a 
joint Gaussian distribution.

A Gaussian Process is completely specified by a mean function and covariance 
function.

We still have a training step and a test step. Below we walk through what it 
means to cast each step into the Gaussian Process framework.

\section{\emph{The Cannon} Training Step in the Gaussian Process Framework}

For each pixel, we optimize for a set of five hyperparameters that fully 
describe the covariance matrix that characterizes the Gaussian process model.

We must begin by selecting the functional form of the covariance between any 
two outputs (in this case, between any two flux values in a spectrum, across 
the label space). We choose a squared exponential covariance function. 

For a set of $N_{ref}$ reference objects $n$, each has a continuum-normalized
flux measurement $f_{n \lambda}$ at wavelength $\lambda$. 

Each training star has $K$ labels $l_{nk}$ 
Say we have a set of $n$ stars, each with $k$ training labels 
$l = l_{n1}, l_{n2}, \cdots, l_{nk}$.

For any given star, the covariance between any pair of flux values 
$ f_{n \lambda_i}$ and $f_{n \lambda_j}$, the 
covariance across 3D label space is

\begin{equation}
  Cov(f_{n \lambda_i}, f_{n \lambda_j}) = a^2 \exp(-\frac{1}{2} \sum_{k=1}^K \frac{(l_{ni}-l_{nj})^2}{\tau_n^2}) +
  \delta_{ij} s^2
\end{equation}

\begin{equation}
  \theta = a, \tau_0, \tau_1, \tau_2, s
\end{equation}

\begin{equation}
  K = a^2 \exp(-\frac{1}{2} \sum_{n=1}^3 \frac{(l_{ni}-l_{nj})^2}{\tau_n^2}) +
  \delta_{ij} s^2
\end{equation}

Note a few things. First, the covariance between outputs is a function of the 
input labels. Second, this covariance approaches unity as the points become 
closer together in label space and becomes smaller as the distance between 
the points increases. Thus the variable $\tau$ can informally be thought of 
as a length scale over which flux values become more ``different`` or contain 
less information about each other.

In a nutshell, we optimize by maximizing the log marginal likelihood for a 
single pixel across all objects. The mathematics of how to do so is laid out 
below\ldots

% (L, flag), alpha = compute_l_and_alpha(theta, f, var, l_all)
% K = kernel(theta, l_all[:, None], l_all[None, :])
% add in the uncertainties from the flux to the kernel
% K[np.diag_indices_from(K)] += var
% L, flag = cho_factor(K, overwrite_a=True)

% lndet = np.sum(np.log(np.diag(L)))
% ln_likelihood = -0.5*np.dot(f, alpha)-lndet

\section{\emph{The Cannon} Test Step in the Gaussian Process Framework}

While the training step took place across pixel space, optimizing for 
hyperparameters across objects at each pixel independently, the test step 
will take place across object space, optimizing for labels across pixels at 
each object independently.

Now, we have a set of $(n_{obj}, n_{l})$ hyperparameters where $n_{obj}$ is 
the number of training objects and $n_{l}$ is the number of training labels 
describing each object.

We write a test log likelihood function that optimizes over the full pixel 
and 3D label space in order to determine, for a particular object, which 
combination of labels is most likely given the measured flux at that pixel.

\section{Results: Comparison with Quadratic Model}

Hopefully we find that it's a hell of a lot better!

\section{Discussion}

One challenge is the computational time. Much slower than simply choosing 
a quadratic model. Whereas that took this much time, the training steps scales 
with the cube of the number of training objects. In this case, it took 20 
hours on a 4-core processor, with the operations running in parallel (all 
pixels are independent.) Similarly, labels for the test objects can be 
evaluated in parallel, and the test step took X hours on a 4-core processor. 

\section{Acknowledgements}

AYQH was partially supported by a Fulbright grant through the German-American
Fulbright Commission.

The research has received funding from the European Research Council under the
European Union's Seventh Framework Programme (FP 7) ERC Grant Agreement n.
[321035].

%\begin{thebibliography}{24}

%\end{thebibliography}

\end{document}


